<html>
<head>
<title>layer_tests.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
.ln { color: #606366; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
layer_tests.py</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">from </span><span class="s1">nndl.layers </span><span class="s0">import </span><span class="s1">*</span>
<a name="l2"><span class="ln">2    </span></a><span class="s0">from </span><span class="s1">cs231n.gradient_check </span><span class="s0">import </span><span class="s1">eval_numerical_gradient</span><span class="s0">, </span><span class="s1">eval_numerical_gradient_array</span>
<a name="l3"><span class="ln">3    </span></a><span class="s0">from </span><span class="s1">nndl.layer_utils </span><span class="s0">import </span><span class="s1">affine_relu_forward</span><span class="s0">, </span><span class="s1">affine_relu_backward</span>
<a name="l4"><span class="ln">4    </span></a><span class="s0">from </span><span class="s1">nndl.fc_net </span><span class="s0">import </span><span class="s1">FullyConnectedNet</span>
<a name="l5"><span class="ln">5    </span></a>
<a name="l6"><span class="ln">6    </span></a><span class="s0">def </span><span class="s1">rel_error(x</span><span class="s0">, </span><span class="s1">y):</span>
<a name="l7"><span class="ln">7    </span></a>  <span class="s2">&quot;&quot;&quot; returns relative error &quot;&quot;&quot;</span>
<a name="l8"><span class="ln">8    </span></a>  <span class="s0">return </span><span class="s1">np.max(np.abs(x - y) / (np.maximum(</span><span class="s3">1e-8</span><span class="s0">, </span><span class="s1">np.abs(x) + np.abs(y))))</span>
<a name="l9"><span class="ln">9    </span></a>
<a name="l10"><span class="ln">10   </span></a><span class="s0">def </span><span class="s1">affine_forward_test():</span>
<a name="l11"><span class="ln">11   </span></a>    <span class="s4"># Test the affine_forward function</span>
<a name="l12"><span class="ln">12   </span></a>
<a name="l13"><span class="ln">13   </span></a>    <span class="s1">num_inputs = </span><span class="s3">2</span>
<a name="l14"><span class="ln">14   </span></a>    <span class="s1">input_shape = (</span><span class="s3">4</span><span class="s0">, </span><span class="s3">5</span><span class="s0">, </span><span class="s3">6</span><span class="s1">)</span>
<a name="l15"><span class="ln">15   </span></a>    <span class="s1">output_dim = </span><span class="s3">3</span>
<a name="l16"><span class="ln">16   </span></a>
<a name="l17"><span class="ln">17   </span></a>    <span class="s1">input_size = num_inputs * np.prod(input_shape)</span>
<a name="l18"><span class="ln">18   </span></a>    <span class="s1">weight_size = output_dim * np.prod(input_shape)</span>
<a name="l19"><span class="ln">19   </span></a>
<a name="l20"><span class="ln">20   </span></a>    <span class="s1">x = np.linspace(-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">num=input_size).reshape(num_inputs</span><span class="s0">, </span><span class="s1">*input_shape)</span>
<a name="l21"><span class="ln">21   </span></a>    <span class="s1">w = np.linspace(-</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.3</span><span class="s0">, </span><span class="s1">num=weight_size).reshape(np.prod(input_shape)</span><span class="s0">, </span><span class="s1">output_dim)</span>
<a name="l22"><span class="ln">22   </span></a>    <span class="s1">b = np.linspace(-</span><span class="s3">0.3</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">num=output_dim)</span>
<a name="l23"><span class="ln">23   </span></a>
<a name="l24"><span class="ln">24   </span></a>    <span class="s1">out</span><span class="s0">, </span><span class="s1">_ = affine_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)</span>
<a name="l25"><span class="ln">25   </span></a>    <span class="s1">correct_out = np.array([[ </span><span class="s3">1.49834967</span><span class="s0">,  </span><span class="s3">1.70660132</span><span class="s0">,  </span><span class="s3">1.91485297</span><span class="s1">]</span><span class="s0">,</span>
<a name="l26"><span class="ln">26   </span></a>                            <span class="s1">[ </span><span class="s3">3.25553199</span><span class="s0">,  </span><span class="s3">3.5141327</span><span class="s0">,   </span><span class="s3">3.77273342</span><span class="s1">]])</span>
<a name="l27"><span class="ln">27   </span></a>
<a name="l28"><span class="ln">28   </span></a>    <span class="s4"># Compare your output with ours. The error should be around 1e-9.</span>
<a name="l29"><span class="ln">29   </span></a>    <span class="s1">print(</span><span class="s5">'If affine_forward function is working, difference should be less than 1e-9:'</span><span class="s1">)</span>
<a name="l30"><span class="ln">30   </span></a>    <span class="s1">print(</span><span class="s5">'difference: {}'</span><span class="s1">.format(rel_error(out</span><span class="s0">, </span><span class="s1">correct_out)))</span>
<a name="l31"><span class="ln">31   </span></a>    
<a name="l32"><span class="ln">32   </span></a><span class="s0">def </span><span class="s1">affine_backward_test(): </span>
<a name="l33"><span class="ln">33   </span></a>    <span class="s4"># Test the affine_backward function</span>
<a name="l34"><span class="ln">34   </span></a>
<a name="l35"><span class="ln">35   </span></a>    <span class="s1">x = np.random.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
<a name="l36"><span class="ln">36   </span></a>    <span class="s1">w = np.random.randn(</span><span class="s3">6</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span>
<a name="l37"><span class="ln">37   </span></a>    <span class="s1">b = np.random.randn(</span><span class="s3">5</span><span class="s1">)</span>
<a name="l38"><span class="ln">38   </span></a>    <span class="s1">dout = np.random.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span>
<a name="l39"><span class="ln">39   </span></a>
<a name="l40"><span class="ln">40   </span></a>    <span class="s1">dx_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">x: affine_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l41"><span class="ln">41   </span></a>    <span class="s1">dw_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">w: affine_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l42"><span class="ln">42   </span></a>    <span class="s1">db_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">b: affine_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">b</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l43"><span class="ln">43   </span></a>
<a name="l44"><span class="ln">44   </span></a>    <span class="s1">_</span><span class="s0">, </span><span class="s1">cache = affine_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)</span>
<a name="l45"><span class="ln">45   </span></a>    <span class="s1">dx</span><span class="s0">, </span><span class="s1">dw</span><span class="s0">, </span><span class="s1">db = affine_backward(dout</span><span class="s0">, </span><span class="s1">cache)</span>
<a name="l46"><span class="ln">46   </span></a>
<a name="l47"><span class="ln">47   </span></a>    <span class="s4"># The error should be around 1e-10</span>
<a name="l48"><span class="ln">48   </span></a>    <span class="s1">print(</span><span class="s5">'If affine_backward is working, error should be less than 1e-9::'</span><span class="s1">)</span>
<a name="l49"><span class="ln">49   </span></a>    <span class="s1">print(</span><span class="s5">'dx error: {}'</span><span class="s1">.format(rel_error(dx_num</span><span class="s0">, </span><span class="s1">dx)))</span>
<a name="l50"><span class="ln">50   </span></a>    <span class="s1">print(</span><span class="s5">'dw error: {}'</span><span class="s1">.format(rel_error(dw_num</span><span class="s0">, </span><span class="s1">dw)))</span>
<a name="l51"><span class="ln">51   </span></a>    <span class="s1">print(</span><span class="s5">'db error: {}'</span><span class="s1">.format(rel_error(db_num</span><span class="s0">, </span><span class="s1">db)))</span>
<a name="l52"><span class="ln">52   </span></a>    
<a name="l53"><span class="ln">53   </span></a><span class="s0">def </span><span class="s1">relu_forward_test():</span>
<a name="l54"><span class="ln">54   </span></a>    <span class="s4"># Test the relu_forward function</span>
<a name="l55"><span class="ln">55   </span></a>
<a name="l56"><span class="ln">56   </span></a>    <span class="s1">x = np.linspace(-</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">num=</span><span class="s3">12</span><span class="s1">).reshape(</span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span>
<a name="l57"><span class="ln">57   </span></a>
<a name="l58"><span class="ln">58   </span></a>    <span class="s1">out</span><span class="s0">, </span><span class="s1">_ = relu_forward(x)</span>
<a name="l59"><span class="ln">59   </span></a>    <span class="s1">correct_out = np.array([[ </span><span class="s3">0.</span><span class="s0">,          </span><span class="s3">0.</span><span class="s0">,          </span><span class="s3">0.</span><span class="s0">,          </span><span class="s3">0.</span><span class="s0">,        </span><span class="s1">]</span><span class="s0">,</span>
<a name="l60"><span class="ln">60   </span></a>                            <span class="s1">[ </span><span class="s3">0.</span><span class="s0">,          </span><span class="s3">0.</span><span class="s0">,          </span><span class="s3">0.04545455</span><span class="s0">,  </span><span class="s3">0.13636364</span><span class="s0">,</span><span class="s1">]</span><span class="s0">,</span>
<a name="l61"><span class="ln">61   </span></a>                            <span class="s1">[ </span><span class="s3">0.22727273</span><span class="s0">,  </span><span class="s3">0.31818182</span><span class="s0">,  </span><span class="s3">0.40909091</span><span class="s0">,  </span><span class="s3">0.5</span><span class="s0">,       </span><span class="s1">]])</span>
<a name="l62"><span class="ln">62   </span></a>
<a name="l63"><span class="ln">63   </span></a>    <span class="s4"># Compare your output with ours. The error should be around 1e-8</span>
<a name="l64"><span class="ln">64   </span></a>    <span class="s1">print(</span><span class="s5">'If relu_forward function is working, difference should be around 1e-8:'</span><span class="s1">)</span>
<a name="l65"><span class="ln">65   </span></a>    <span class="s1">print(</span><span class="s5">'difference: {}'</span><span class="s1">.format(rel_error(out</span><span class="s0">, </span><span class="s1">correct_out)))</span>
<a name="l66"><span class="ln">66   </span></a>    
<a name="l67"><span class="ln">67   </span></a><span class="s0">def </span><span class="s1">relu_backward_test():</span>
<a name="l68"><span class="ln">68   </span></a>    <span class="s1">x = np.random.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
<a name="l69"><span class="ln">69   </span></a>    <span class="s1">dout = np.random.randn(*x.shape)</span>
<a name="l70"><span class="ln">70   </span></a>
<a name="l71"><span class="ln">71   </span></a>    <span class="s1">dx_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">x: relu_forward(x)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l72"><span class="ln">72   </span></a>
<a name="l73"><span class="ln">73   </span></a>    <span class="s1">_</span><span class="s0">, </span><span class="s1">cache = relu_forward(x)</span>
<a name="l74"><span class="ln">74   </span></a>    <span class="s1">dx = relu_backward(dout</span><span class="s0">, </span><span class="s1">cache)</span>
<a name="l75"><span class="ln">75   </span></a>
<a name="l76"><span class="ln">76   </span></a>    <span class="s4"># The error should be around 1e-12</span>
<a name="l77"><span class="ln">77   </span></a>    <span class="s1">print(</span><span class="s5">'If relu_forward function is working, error should be less than 1e-9:'</span><span class="s1">)</span>
<a name="l78"><span class="ln">78   </span></a>    <span class="s1">print(</span><span class="s5">'dx error: {}'</span><span class="s1">.format(rel_error(dx_num</span><span class="s0">, </span><span class="s1">dx)))</span>
<a name="l79"><span class="ln">79   </span></a>    
<a name="l80"><span class="ln">80   </span></a><span class="s0">def </span><span class="s1">affine_relu_test():</span>
<a name="l81"><span class="ln">81   </span></a>
<a name="l82"><span class="ln">82   </span></a>    <span class="s1">x = np.random.randn(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span>
<a name="l83"><span class="ln">83   </span></a>    <span class="s1">w = np.random.randn(</span><span class="s3">12</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
<a name="l84"><span class="ln">84   </span></a>    <span class="s1">b = np.random.randn(</span><span class="s3">10</span><span class="s1">)</span>
<a name="l85"><span class="ln">85   </span></a>    <span class="s1">dout = np.random.randn(</span><span class="s3">2</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
<a name="l86"><span class="ln">86   </span></a>
<a name="l87"><span class="ln">87   </span></a>    <span class="s1">out</span><span class="s0">, </span><span class="s1">cache = affine_relu_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)</span>
<a name="l88"><span class="ln">88   </span></a>    <span class="s1">dx</span><span class="s0">, </span><span class="s1">dw</span><span class="s0">, </span><span class="s1">db = affine_relu_backward(dout</span><span class="s0">, </span><span class="s1">cache)</span>
<a name="l89"><span class="ln">89   </span></a>
<a name="l90"><span class="ln">90   </span></a>    <span class="s1">dx_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">x: affine_relu_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l91"><span class="ln">91   </span></a>    <span class="s1">dw_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">w: affine_relu_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l92"><span class="ln">92   </span></a>    <span class="s1">db_num = eval_numerical_gradient_array(</span><span class="s0">lambda </span><span class="s1">b: affine_relu_forward(x</span><span class="s0">, </span><span class="s1">w</span><span class="s0">, </span><span class="s1">b)[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">b</span><span class="s0">, </span><span class="s1">dout)</span>
<a name="l93"><span class="ln">93   </span></a>
<a name="l94"><span class="ln">94   </span></a>    <span class="s1">print(</span><span class="s5">'If affine_relu_forward and affine_relu_backward are working, error should be less than 1e-9::'</span><span class="s1">)</span>
<a name="l95"><span class="ln">95   </span></a>    <span class="s1">print(</span><span class="s5">'dx error: {}'</span><span class="s1">.format(rel_error(dx_num</span><span class="s0">, </span><span class="s1">dx)))</span>
<a name="l96"><span class="ln">96   </span></a>    <span class="s1">print(</span><span class="s5">'dw error: {}'</span><span class="s1">.format(rel_error(dw_num</span><span class="s0">, </span><span class="s1">dw)))</span>
<a name="l97"><span class="ln">97   </span></a>    <span class="s1">print(</span><span class="s5">'db error: {}'</span><span class="s1">.format(rel_error(db_num</span><span class="s0">, </span><span class="s1">db)))</span>
<a name="l98"><span class="ln">98   </span></a>    
<a name="l99"><span class="ln">99   </span></a><span class="s0">def </span><span class="s1">fc_net_test():</span>
<a name="l100"><span class="ln">100  </span></a>    <span class="s1">N</span><span class="s0">, </span><span class="s1">D</span><span class="s0">, </span><span class="s1">H1</span><span class="s0">, </span><span class="s1">H2</span><span class="s0">, </span><span class="s1">C = </span><span class="s3">2</span><span class="s0">, </span><span class="s3">15</span><span class="s0">, </span><span class="s3">20</span><span class="s0">, </span><span class="s3">30</span><span class="s0">, </span><span class="s3">10</span>
<a name="l101"><span class="ln">101  </span></a>    <span class="s1">X = np.random.randn(N</span><span class="s0">, </span><span class="s1">D)</span>
<a name="l102"><span class="ln">102  </span></a>    <span class="s1">y = np.random.randint(C</span><span class="s0">, </span><span class="s1">size=(N</span><span class="s0">,</span><span class="s1">))</span>
<a name="l103"><span class="ln">103  </span></a>
<a name="l104"><span class="ln">104  </span></a>    <span class="s0">for </span><span class="s1">reg </span><span class="s0">in </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">3.14</span><span class="s1">]:</span>
<a name="l105"><span class="ln">105  </span></a>      <span class="s1">print(</span><span class="s5">'Running check with reg = {}'</span><span class="s1">.format(reg))</span>
<a name="l106"><span class="ln">106  </span></a>      <span class="s1">model = FullyConnectedNet([H1</span><span class="s0">, </span><span class="s1">H2]</span><span class="s0">, </span><span class="s1">input_dim=D</span><span class="s0">, </span><span class="s1">num_classes=C</span><span class="s0">,</span>
<a name="l107"><span class="ln">107  </span></a>                                <span class="s1">reg=reg</span><span class="s0">, </span><span class="s1">weight_scale=</span><span class="s3">5e-2</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
<a name="l108"><span class="ln">108  </span></a>
<a name="l109"><span class="ln">109  </span></a>      <span class="s1">loss</span><span class="s0">, </span><span class="s1">grads = model.loss(X</span><span class="s0">, </span><span class="s1">y)</span>
<a name="l110"><span class="ln">110  </span></a>      <span class="s1">print(</span><span class="s5">'Initial loss: {}'</span><span class="s1">.format(loss))</span>
<a name="l111"><span class="ln">111  </span></a>
<a name="l112"><span class="ln">112  </span></a>      <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">sorted(grads):</span>
<a name="l113"><span class="ln">113  </span></a>        <span class="s1">f = </span><span class="s0">lambda </span><span class="s1">_: model.loss(X</span><span class="s0">, </span><span class="s1">y)[</span><span class="s3">0</span><span class="s1">]</span>
<a name="l114"><span class="ln">114  </span></a>        <span class="s1">grad_num = eval_numerical_gradient(f</span><span class="s0">, </span><span class="s1">model.params[name]</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s0">False, </span><span class="s1">h=</span><span class="s3">1e-5</span><span class="s1">)</span>
<a name="l115"><span class="ln">115  </span></a>        <span class="s1">print(</span><span class="s5">'{} relative error: {}'</span><span class="s1">.format(name</span><span class="s0">, </span><span class="s1">rel_error(grad_num</span><span class="s0">, </span><span class="s1">grads[name])))</span></pre>
</body>
</html>